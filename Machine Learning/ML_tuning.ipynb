{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we will tune the various ML algorithms for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion of text data into representations readable by ML algorithms\n",
    "\n",
    "Before we utitlize machine learning models for prediction, we need to convert the text data into a form that ML models can read.\n",
    "Here are some forms we will explore:\n",
    "\n",
    "1. Bag of Words representation\n",
    "2. TF-IDF representation\n",
    "3. Word2Vec representation of text\n",
    "4. GloVe representation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim\n",
    "%pip install scipy==1.12.0\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.data import find\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/ML/ML_train.csv')\n",
    "test_df = pd.read_csv('data/ML/ML_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['text']\n",
    "y_train = train_df['humor']\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['humor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "bow_vectorizer.fit(X_train)\n",
    "\n",
    "bow_X_train = bow_vectorizer.transform(X_train)\n",
    "bow_X_test = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "tfidf_X_train = tfidf_vectorizer.transform(X_train)\n",
    "tfidf_X_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec: Using a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "path = api.load('word2vec-google-news-300', return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as each piece of text has a different length, we need to use a function to average the\n",
    "# vector representation of each word in the vector, so that every vector will have the same length\n",
    "\n",
    "def sent_vec(sent, model):\n",
    "    vector_size = model.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "      if w in model:\n",
    "        ctr += 1\n",
    "        wv_res += model[w]\n",
    "    wv_res = wv_res / ctr\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we cleaned our text data previously, we only need to convert the text in each row into lists, where each element is a token\n",
    "\n",
    "def split_text(text):\n",
    "  return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['text'].apply(split_text)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tokens'] = test_df['text'].apply(split_text)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['w2v'] = train_df['tokens'].apply(lambda x: sent_vec(x, model))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['w2v'] = test_df['tokens'].apply(lambda x: sent_vec(x, model))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X_train = train_df['w2v'].to_list()\n",
    "w2v_X_test = test_df['w2v'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe: We will use a pretrained model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['glove'] = train_df['tokens'].apply(lambda x: sent_vec(x, model))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['glove'] = test_df['tokens'].apply(lambda x: sent_vec(x, model))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_X_train = train_df['glove'].to_list()\n",
    "glove_X_test = test_df['glove'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, trainX, trainY, testX, testY):\n",
    "\n",
    "    # training the model\n",
    "    fitted_model = model.fit(trainX, trainY)\n",
    "\n",
    "    # getting predictions\n",
    "    y_preds_train = fitted_model.predict(trainX)\n",
    "    y_preds_test = fitted_model.predict(testX)\n",
    "\n",
    "    # evaluating the model\n",
    "    print()\n",
    "    print(model)\n",
    "    print(f\"Train accuracy score : {accuracy_score(trainY, y_preds_train)}\")\n",
    "    print(f\"Test accuracy score : {accuracy_score(testY, y_preds_test)}\")\n",
    "    print(classification_report(testY, y_preds_test))\n",
    "    print('\\n',40*'-')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
