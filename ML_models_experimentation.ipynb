{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq36wyj8Cgkb"
      },
      "source": [
        "## In this notebook, we will explore various ML algorithms\n",
        "\n",
        "1. Random forest\n",
        "2. Naive Bayes\n",
        "3. Logistic Regression\n",
        "4. Support Vector Machines\n",
        "5. Gradient Boosted Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Maxi4uCgkc"
      },
      "source": [
        "## Converting text data into forms that ML models can read\n",
        "\n",
        "Before we utitlize machine learning models for prediction, we need to convert the text data into a form that ML models can read.\n",
        "Here are some forms we will explore:\n",
        "\n",
        "1. Word2Vec representation of text (for this, it might be better to not remove stopwords: https://www.kaggle.com/code/harshitmakkar/nlp-word2vec)\n",
        "2. Bag of Words representation\n",
        "3. TF-IDF representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lNkDZeBCgkd",
        "outputId": "d69a322a-e5aa-458d-ecb2-d75cac581a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Collecting scipy==1.12.0\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.29.0,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy==1.12.0) (1.25.2)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "Successfully installed scipy-1.12.0\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.12.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim\n",
        "%pip install scipy==1.12.0\n",
        "%pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KdbeGfPOCgke"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.data import find\n",
        "import gensim\n",
        "import nltk\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3daZhMCgkf"
      },
      "source": [
        "Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DdUVii-CCgkf"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('data/ML/ML_train.csv')\n",
        "test_df = pd.read_csv('data/ML/ML_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EXEYErrSCgkf",
        "outputId": "484e0bfa-2599-4b60-c411-e5b3e9435e85"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>humor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>watch swimmer disappear winter storm jonas</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>laughed reagan trump idea outlast political stage</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hey cold go corner 90 degress</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cant get standing desk almost good</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wanna hear joke penis never mind long</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  humor\n",
              "0         watch swimmer disappear winter storm jonas  False\n",
              "1  laughed reagan trump idea outlast political stage  False\n",
              "2                      hey cold go corner 90 degress   True\n",
              "3                 cant get standing desk almost good  False\n",
              "4              wanna hear joke penis never mind long   True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yg1GvWxACgkf",
        "outputId": "3ae8cf99-ebaf-4134-d5c7-305775a2d107"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>humor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thought reddit joke today triangle rectangle f...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>much pirate pay corn buck ear</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hillary clinton sent book every gop candidatee...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>italian union lambast new museum bos working hard</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>life ocean surface wholly depends live</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  humor\n",
              "0  thought reddit joke today triangle rectangle f...   True\n",
              "1                      much pirate pay corn buck ear   True\n",
              "2  hillary clinton sent book every gop candidatee...  False\n",
              "3  italian union lambast new museum bos working hard  False\n",
              "4             life ocean surface wholly depends live  False"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xmtiDZLXCgkg"
      },
      "outputs": [],
      "source": [
        "X_train = train_df['text']\n",
        "y_train = train_df['humor']\n",
        "X_test = test_df['text']\n",
        "y_test = test_df['humor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDiER4NdCgkg"
      },
      "source": [
        "Word2Vec: We will use a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsH8tvUNCgkg",
        "outputId": "dedb27a2-6f58-4e00-8b28-32fa682d13e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.\n",
            "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\n",
            "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
            "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
            "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
            "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\n",
            "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "# path = api.load(\"word2vec-google-news-300\", return_path = True)\n",
        "import json\n",
        "info = api.info()\n",
        "for model_name, model_data in sorted(info['models'].items()):\n",
        "    print(\n",
        "        '%s (%d records): %s' % (\n",
        "            model_name,\n",
        "            model_data.get('num_records', -1),\n",
        "            model_data['description'],\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctj0XHzdDHTS",
        "outputId": "d71f1fcd-65c9-44e4-857f-88ba6ae4a235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "path = api.load('word2vec-google-news-300', return_path=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6ObVxvALCgkh"
      },
      "outputs": [],
      "source": [
        "model = KeyedVectors.load_word2vec_format(path, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WiyirFV_GDr1"
      },
      "outputs": [],
      "source": [
        "# Use a function to vectorize the sentences into Word2Vec form\n",
        "\n",
        "def get_vector(tokens,vector,k=300):\n",
        "    if len(tokens)<1:\n",
        "        return np.zeros(k)\n",
        "\n",
        "    vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens]\n",
        "    lens = len(vectorized)\n",
        "    sums = np.sum(vectorized,axis=0)\n",
        "    avg = np.divide(sums,lens)\n",
        "    return avg\n",
        "\n",
        "\n",
        "def get_embedding(vectors,text,k=300):\n",
        "    embs = text.apply(lambda x:get_vector(x,vectors,k=300))\n",
        "    return list(embs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ir9R52hqJKez"
      },
      "outputs": [],
      "source": [
        "w2v_X_train = get_embedding(model, train_df['text'], k=300)\n",
        "w2v_X_test = get_embedding(model, test_df['text'], k=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GzcC1JQCgki"
      },
      "source": [
        "Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoBumSO3Cgki"
      },
      "outputs": [],
      "source": [
        "bow_vectorizer = CountVectorizer()\n",
        "bow_vectorizer.fit(X_train)\n",
        "\n",
        "bow_X_train = bow_vectorizer.transform(X_train)\n",
        "bow_X_test = bow_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNdpiGNQCgki"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXSd3_HuCgki"
      },
      "outputs": [],
      "source": [
        "# ngram_range=(1, 3), min_df=2, max_df=0.85\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(X_train)\n",
        "\n",
        "tfidf_X_train = tfidf_vectorizer.transform(X_train)\n",
        "tfidf_X_test = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RD3QE1hCgki"
      },
      "source": [
        "# Function to test the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NIrOaiecCgki"
      },
      "outputs": [],
      "source": [
        "def train_and_eval(model, trainX, trainY, testX, testY):\n",
        "\n",
        "    # training the model\n",
        "    fitted_model = model.fit(trainX, trainY)\n",
        "\n",
        "    # getting predictions\n",
        "    y_preds_train = fitted_model.predict(trainX)\n",
        "    y_preds_test = fitted_model.predict(testX)\n",
        "\n",
        "    # evaluating the model\n",
        "    print()\n",
        "    print(model)\n",
        "    print(f\"Train accuracy score : {accuracy_score(trainY, y_preds_train)}\")\n",
        "    print(f\"Test accuracy score : {accuracy_score(testY, y_preds_test)}\")\n",
        "    print(classification_report(testY, y_preds_test))\n",
        "    print('\\n',40*'-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRzvueejCgkj"
      },
      "source": [
        "# Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN717T9eCgkj"
      },
      "source": [
        "Multinomial Naive Bayes with BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwZpj_a6Cgkj",
        "outputId": "2abec595-096e-4fdd-bc14-f3011a53da06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MultinomialNB()\n",
            "Train accuracy score : 0.9181125\n",
            "Test accuracy score : 0.901575\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.91      0.89      0.90     20000\n",
            "        True       0.89      0.91      0.90     20000\n",
            "\n",
            "    accuracy                           0.90     40000\n",
            "   macro avg       0.90      0.90      0.90     40000\n",
            "weighted avg       0.90      0.90      0.90     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "nb_model = MultinomialNB()\n",
        "train_and_eval(nb_model, bow_X_train, y_train, bow_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXu4vun0Cgkj"
      },
      "source": [
        "Multinomial Naive Bayes with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m83aVe0PCgkj",
        "outputId": "5b9d1512-1caf-45ec-89f7-cd94c7fb4f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MultinomialNB()\n",
            "Train accuracy score : 0.9185125\n",
            "Test accuracy score : 0.899375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.91      0.89      0.90     20000\n",
            "        True       0.89      0.91      0.90     20000\n",
            "\n",
            "    accuracy                           0.90     40000\n",
            "   macro avg       0.90      0.90      0.90     40000\n",
            "weighted avg       0.90      0.90      0.90     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "nb_model = MultinomialNB()\n",
        "train_and_eval(nb_model, tfidf_X_train, y_train, tfidf_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Xq3DfXMuBl"
      },
      "source": [
        "Multinomial Naive Bayes with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8gwaeMVK7k1",
        "outputId": "3cbbfda9-7b0e-4365-d0b3-16d5bfef7fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MultinomialNB()\n",
            "Train accuracy score : 0.5845\n",
            "Test accuracy score : 0.5841\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.58      0.62      0.60     20000\n",
            "        True       0.59      0.55      0.57     20000\n",
            "\n",
            "    accuracy                           0.58     40000\n",
            "   macro avg       0.58      0.58      0.58     40000\n",
            "weighted avg       0.58      0.58      0.58     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "nb_model = MultinomialNB()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# transform the inputs because Multinomial Naive Bayes does not take in negative inputs\n",
        "scaler.fit(w2v_X_train)\n",
        "nb_w2v_train = scaler.transform(w2v_X_train)\n",
        "nb_w2v_test = scaler.transform(w2v_X_test)\n",
        "\n",
        "train_and_eval(nb_model, nb_w2v_train, y_train, nb_w2v_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-oTiooACgkj"
      },
      "source": [
        "BoW works best for the Multinomial Naive Bayes model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsheDyCxCgkk"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0EE-0wECgkk"
      },
      "source": [
        "Logistic Regression with BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi9wARnhCgkk",
        "outputId": "a9a6e2a9-b5a4-4935-9f75-25f282a229df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LogisticRegression(random_state=42)\n",
            "Train accuracy score : 0.93783125\n",
            "Test accuracy score : 0.90545\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.91      0.90      0.91     20000\n",
            "        True       0.90      0.91      0.91     20000\n",
            "\n",
            "    accuracy                           0.91     40000\n",
            "   macro avg       0.91      0.91      0.91     40000\n",
            "weighted avg       0.91      0.91      0.91     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "log_model = LogisticRegression(random_state=42)\n",
        "train_and_eval(log_model, bow_X_train, y_train, bow_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU0HTXNxCgkk"
      },
      "source": [
        "Logistic Regression with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZ5cO0OCgkk",
        "outputId": "22e1056d-795a-4e07-ffdd-984c9522f163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LogisticRegression(random_state=42)\n",
            "Train accuracy score : 0.91860625\n",
            "Test accuracy score : 0.9008\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.90      0.90      0.90     20000\n",
            "        True       0.90      0.90      0.90     20000\n",
            "\n",
            "    accuracy                           0.90     40000\n",
            "   macro avg       0.90      0.90      0.90     40000\n",
            "weighted avg       0.90      0.90      0.90     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "log_model = LogisticRegression(random_state=42)\n",
        "train_and_eval(log_model, tfidf_X_train, y_train, tfidf_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xvBelaGM0c7"
      },
      "source": [
        "Logistic Regression with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8q5GmuSM3dz",
        "outputId": "cfd769e8-2393-407a-98d0-41250108cd4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LogisticRegression(random_state=42)\n",
            "Train accuracy score : 0.6046875\n",
            "Test accuracy score : 0.601375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.60      0.62      0.61     20000\n",
            "        True       0.61      0.58      0.59     20000\n",
            "\n",
            "    accuracy                           0.60     40000\n",
            "   macro avg       0.60      0.60      0.60     40000\n",
            "weighted avg       0.60      0.60      0.60     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "log_model = LogisticRegression(random_state=42)\n",
        "train_and_eval(log_model, w2v_X_train, y_train, w2v_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx6K7eaINC6E"
      },
      "source": [
        "Logistic Regression works best with BoW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T07btxZ0Cgkk"
      },
      "source": [
        "# Random Forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY_6jXGACgkl"
      },
      "source": [
        "Random Forest with BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ebxx9BiCgkl",
        "outputId": "e3888f5a-35b9-4b28-9db8-df65afd645db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RandomForestClassifier(random_state=42)\n",
            "Train accuracy score : 0.99999375\n",
            "Test accuracy score : 0.87755\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.89      0.86      0.88     20000\n",
            "        True       0.86      0.90      0.88     20000\n",
            "\n",
            "    accuracy                           0.88     40000\n",
            "   macro avg       0.88      0.88      0.88     40000\n",
            "weighted avg       0.88      0.88      0.88     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(random_state=42)\n",
        "train_and_eval(clf, bow_X_train, y_train, bow_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SERne6jNCgkl"
      },
      "source": [
        "Random Forest with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBvPJYuTCgkl",
        "outputId": "04293add-6e1c-4015-d48d-e5e66c947501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RandomForestClassifier(random_state=42)\n",
            "Train accuracy score : 0.99999375\n",
            "Test accuracy score : 0.874625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.89      0.85      0.87     20000\n",
            "        True       0.86      0.90      0.88     20000\n",
            "\n",
            "    accuracy                           0.87     40000\n",
            "   macro avg       0.88      0.87      0.87     40000\n",
            "weighted avg       0.88      0.87      0.87     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(random_state=42)\n",
        "train_and_eval(clf, tfidf_X_train, y_train, tfidf_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibcMe1FdNILa"
      },
      "source": [
        "Random Forest with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aadTEvpkNIAT",
        "outputId": "9ebcd5b8-6751-4e4c-f69c-848f6333a8aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RandomForestClassifier(random_state=42)\n",
            "Train accuracy score : 1.0\n",
            "Test accuracy score : 0.625325\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.62      0.63      0.63     20000\n",
            "        True       0.63      0.62      0.62     20000\n",
            "\n",
            "    accuracy                           0.63     40000\n",
            "   macro avg       0.63      0.63      0.63     40000\n",
            "weighted avg       0.63      0.63      0.63     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(random_state=42)\n",
        "train_and_eval(clf, w2v_X_train, y_train, w2v_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFDERMwwRSHU"
      },
      "source": [
        "Random Forest works best with BoW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx5OCvsrCgkl"
      },
      "source": [
        "# Gradient Boosted Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRkQBlZfCgkp"
      },
      "source": [
        "XGB classifier with BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW90ZH2ZCgkp",
        "outputId": "a37d13da-ed10-47e6-c203-1491986a95ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
            "              num_parallel_tree=None, random_state=42, ...)\n",
            "Train accuracy score : 0.836325\n",
            "Test accuracy score : 0.8283\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.80      0.88      0.84     20000\n",
            "        True       0.87      0.78      0.82     20000\n",
            "\n",
            "    accuracy                           0.83     40000\n",
            "   macro avg       0.83      0.83      0.83     40000\n",
            "weighted avg       0.83      0.83      0.83     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "xgb = XGBClassifier(random_state=42)\n",
        "train_and_eval(xgb, bow_X_train, y_train, bow_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jis1CmdjCgkq"
      },
      "source": [
        "XGB classifier with TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNuYAUxoCgkq",
        "outputId": "a5a466e5-9fa7-457b-81e4-7c9c5bc3d80e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
            "              num_parallel_tree=None, random_state=42, ...)\n",
            "Train accuracy score : 0.8391625\n",
            "Test accuracy score : 0.8272\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.79      0.88      0.84     20000\n",
            "        True       0.87      0.77      0.82     20000\n",
            "\n",
            "    accuracy                           0.83     40000\n",
            "   macro avg       0.83      0.83      0.83     40000\n",
            "weighted avg       0.83      0.83      0.83     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "xgb = XGBClassifier(random_state=42)\n",
        "train_and_eval(xgb, tfidf_X_train, y_train, tfidf_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a82xmjnRNPzD"
      },
      "source": [
        "XGB classifier with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3jA8YT0NTEj",
        "outputId": "02e7b273-e152-4d90-815a-d5bb58144aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
            "              num_parallel_tree=None, random_state=42, ...)\n",
            "Train accuracy score : 0.75400625\n",
            "Test accuracy score : 0.628825\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.62      0.66      0.64     20000\n",
            "        True       0.64      0.59      0.62     20000\n",
            "\n",
            "    accuracy                           0.63     40000\n",
            "   macro avg       0.63      0.63      0.63     40000\n",
            "weighted avg       0.63      0.63      0.63     40000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "xgb = XGBClassifier(random_state=42)\n",
        "train_and_eval(xgb, w2v_X_train, y_train, w2v_X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuqZJ-zERYDs"
      },
      "source": [
        "XGB classifier works best with BoW"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
