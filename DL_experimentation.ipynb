{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I attempt to use Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long-Short Term Memory (LSTM) to conduct classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchtext/transforms.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchtext/functional.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7ca21348af20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
    "import torchtext\n",
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "# from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch this swimmer disappear into winter storm...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they laughed at reagan, too: trump's ideas wil...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey, are you cold? go over to the corner, it i...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cannot get a standing desk? these are almost a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want to hear a joke about my penis? never mind...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  watch this swimmer disappear into winter storm...  False\n",
       "1  they laughed at reagan, too: trump's ideas wil...  False\n",
       "2  hey, are you cold? go over to the corner, it i...   True\n",
       "3  cannot get a standing desk? these are almost a...  False\n",
       "4  want to hear a joke about my penis? never mind...   True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/DL/DL_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thought up a reddit joke today. when is a tria...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how much do pirates pay for corn? a buck an ear!</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hillary clinton sent her book to every gop can...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian unions lambast new museum boss for wor...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>life below the ocean’s surface wholly depends ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  thought up a reddit joke today. when is a tria...   True\n",
       "1   how much do pirates pay for corn? a buck an ear!   True\n",
       "2  hillary clinton sent her book to every gop can...  False\n",
       "3  italian unions lambast new museum boss for wor...  False\n",
       "4  life below the ocean’s surface wholly depends ...  False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/DL/DL_test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 300\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftMax(dim=1)\n",
    "    \n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
    "\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom dataset should inherit Dataset and override the following methods:\n",
    "1. \\_\\_len\\_\\_ so that len(dataset) returns the size of the dataset.\n",
    "2. \\_\\_getitem\\_\\_ to support the indexing such that dataset[i] can be used to get the ith sample.\n",
    "\n",
    "Need to further preprocess the text data before we can convert the data into a tensor: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial\n",
    "\n",
    "Also, preprocessing the text data: https://medium.com/@theDrewDag/convert-texts-into-tensors-for-deep-learning-74b0cf48d416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\n",
    "\n",
    "train_data_pipe = dp.iter.IterableWrapper(['data/DL/DL_train.csv']) # creating an iterable of filenames\n",
    "train_data_pipe = dp.iter.FileOpener(train_data_pipe, mode='rb') # pass the iterable to FileOpener which then opens the file in read mode\n",
    "train_data_pipe = train_data_pipe.parse_csv(skip_lines=1, delimiter=',', as_tuple=True) # parse the file, which again returns an iterable of tuples representing each rows of the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('watch this swimmer disappear into winter storm jonas', 'False')\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data_pipe:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'good', 'day', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(engTokenize(\"Have a good day!!!\")) # the function is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(data_iter):\n",
    "    for text, humor in data_iter:\n",
    "        yield engTokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <sos> for start of sentence\n",
    "# <eos> for end of sentence\n",
    "# <unk> for unknown words. An example of unknown word is the one skipped because of min_freq=2\n",
    "# <pad> is the padding token. While training, a model we mostly train in batches. In a batch, there can be sentences of different length.\n",
    "# So, we pad the shorter sentences with <pad> token to make length of all sequences in the batch equal.\n",
    "\n",
    "train_vocab = build_vocab_from_iterator(\n",
    "    getTokens(train_data_pipe),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# if some words are not in the vocabulary, we will use <unk> instead of that unknown word\n",
    "train_vocab.set_default_index(train_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, we build the vocab for the test data\n",
    "\n",
    "test_data_pipe = dp.iter.IterableWrapper(['data/DL/DL_test.csv']) # creating an iterable of filenames\n",
    "test_data_pipe = dp.iter.FileOpener(test_data_pipe, mode='rb') # pass the iterable to FileOpener which then opens the file in read mode\n",
    "test_data_pipe = test_data_pipe.parse_csv(skip_lines=1, delimiter=',', as_tuple=True) # parse the file, which again returns an iterable of tuples representing each rows of the csv file\n",
    "\n",
    "test_vocab = build_vocab_from_iterator(\n",
    "    getTokens(test_data_pipe),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# if some words are not in the vocabulary, we will use <unk> instead of that unknown word\n",
    "test_vocab.set_default_index(train_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', 'the', 'a', '.', '?', 'to']\n",
      "['<pad>', '<sos>', '<eos>', '<unk>', 'the', 'a', '.', '?', 'to']\n"
     ]
    }
   ],
   "source": [
    "# train_vocab.get_itos() returns a list with tokens at index based on vocabulary\n",
    "print(train_vocab.get_itos()[:9])\n",
    "print(test_vocab.get_itos()[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after building the vocabulary, we will convert our sentences into corresponding indices\n",
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_transform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentence=how to keep your feet in good shape this summer\n",
      "Transformed sentence=[1, 32, 8, 227, 35, 969, 14, 116, 2520, 53, 427, 2]\n",
      "<sos> how to cross your skin in should survive will summer <eos> "
     ]
    }
   ],
   "source": [
    "# try the above function on a random sentence\n",
    "\n",
    "temp_list = list(train_data_pipe)\n",
    "some_sentence = temp_list[798][0]\n",
    "print(\"Some sentence=\", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(train_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence=\", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = test_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        getTransform(train_vocab)(engTokenize(sequence_pair[0])),\n",
    "        sequence_pair[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 205, 53, 7860, 6221, 86, 791, 1882, 7533, 2], 'False')\n"
     ]
    }
   ],
   "source": [
    "train_data_pipe = train_data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "temp_list = list(train_data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 425, 65, 5, 558, 114, 194, 6, 49, 9, 5, 5170, 5, 30083, 7, 49, 21, 2428, 2], 'True')\n"
     ]
    }
   ],
   "source": [
    "test_data_pipe = test_data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "temp_list = list(test_data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While working for sequence to sequence models, it is recommended to keep the length of sequences in a batch similar.\n",
    "# For that we will use bucketbatch function of data_pipe.\n",
    "\n",
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 538, 1753, 3787, 235, 4152, 10, 68, 1009, 3631, 113, 2679, 2], 'False'), ([1, 126, 183, 20, 51, 958, 23, 4, 119, 41, 56, 14653, 2], 'False'), ([1, 87, 1624, 8329, 23, 1531, 11363, 10, 3962, 6164, 7284, 315, 2], 'False'), ([1, 1410, 204, 1714, 18, 145, 156, 41, 56, 20, 151, 11, 2], 'False'), ([1, 591, 1219, 733, 237, 7119, 8, 863, 5862, 14714, 703, 1896, 2], 'False'), ([1, 3005, 1004, 1376, 9978, 13, 9529, 13, 1955, 107, 111, 1574, 2], 'False'), ([1, 707, 953, 342, 42, 4, 127, 795, 1753, 10, 5706, 388, 2], 'False'), ([1, 5, 1307, 1698, 1285, 6139, 9, 5, 5717, 465, 3190, 6, 2], 'True'), ([1, 16, 19, 6798, 232, 25, 89, 349, 7, 3460, 2933, 28, 2], 'True'), ([1, 409, 2003, 96, 34, 5, 784, 70, 14, 7274, 213, 524, 2], 'False'), ([1, 692, 11, 304, 45, 798, 212, 10, 129, 9, 67, 6, 2], 'True'), ([1, 1282, 47, 23, 6736, 8, 187, 656, 3634, 2384, 23, 1293, 2], 'False'), ([1, 1073, 3414, 2918, 313, 84, 56, 24, 4184, 130, 22, 233, 4461, 2], 'False'), ([1, 3044, 3851, 2272, 137, 8, 5, 234, 17, 21, 37, 1022, 1724, 2], 'False'), ([1, 8692, 2738, 54, 176, 23, 13, 5851, 13, 251, 625, 1808, 3275, 2], 'False'), ([1, 194, 22, 3, 26, 76, 10714, 17, 11836, 76, 5, 519, 234, 2], 'False'), ([1, 16, 33, 4, 9402, 225, 52, 12538, 433, 7, 4793, 10457, 28, 2], 'True'), ([1, 889, 3578, 565, 6667, 23, 69, 4400, 8, 3, 39, 72, 38, 2], 'False'), ([1, 356, 5532, 5262, 18, 9987, 1243, 11, 387, 34, 4, 356, 2182, 2], 'False'), ([1, 16, 9, 327, 6547, 22, 769, 4205, 7, 5385, 2630, 1717, 28, 2], 'True'), ([1, 2102, 2419, 183, 2912, 927, 8278, 10, 879, 8, 259, 25, 3970, 2], 'False'), ([1, 30, 59, 344, 743, 45, 311, 7, 50, 388, 9, 97, 6, 2], 'True'), ([1, 136, 4, 2132, 18204, 23, 193, 13, 100, 2124, 13, 5732, 2814, 2], 'False'), ([1, 238, 191, 384, 60, 46, 213, 1419, 307, 26341, 29, 52, 1413, 2], 'False'), ([1, 5898, 42, 4, 1373, 43, 20, 1180, 15, 11, 71, 1251, 57, 2], 'True'), ([1, 64, 12, 2566, 8, 793, 11, 10, 56, 11, 101, 309, 7, 2], 'True'), ([1, 589, 10, 1802, 1872, 23, 461, 119, 22, 4750, 2840, 14, 982, 2], 'False'), ([1, 32, 19, 11, 465, 80, 3016, 7, 49, 11, 26, 4, 5392, 2], 'True'), ([1, 30, 37, 4354, 664, 8, 4, 1071, 7, 40, 37, 4518, 6, 2], 'True'), ([1, 4, 67, 2540, 36, 96, 209, 11, 527, 290, 17, 200, 222, 2], 'False'), ([1, 8420, 18, 13458, 6461, 23, 14918, 14, 2686, 9, 13, 1189, 13, 2], 'False'), ([1, 16, 19, 11, 47, 5, 3616, 15, 905, 7, 5, 14769, 6, 2], 'True'), ([1, 1139, 2287, 8, 58, 5, 1011, 206, 568, 407, 8, 332, 5829, 2], 'False'), ([1, 193, 72, 26, 5, 6386, 2996, 15, 1597, 8232, 161, 4, 381, 2], 'False'), ([1, 525, 9, 20, 415, 53, 111, 28, 170, 554, 52, 12983, 28, 2], 'True'), ([1, 1528, 45, 5964, 2579, 9, 120, 117, 8, 731, 132, 869, 7, 2], 'True'), ([1, 16, 19, 11, 47, 5, 835, 1909, 1524, 7, 5, 4569, 3, 2], 'True'), ([1, 1206, 3, 1158, 30, 40, 812, 155, 3713, 83, 2134, 507, 706, 2], 'False'), ([1, 4335, 3488, 195, 9793, 23, 20530, 625, 3207, 3, 55, 296, 30120, 2], 'False'), ([1, 61, 99, 131, 55, 1471, 5, 9691, 36, 160, 6659, 57, 65, 2], 'True'), ([1, 16, 19, 11, 47, 5, 2098, 4915, 7, 5, 3246, 3674, 6, 2], 'True'), ([1, 13, 5, 11688, 14, 92, 13, 17, 4, 8547, 15, 169, 104, 2], 'False'), ([1, 145, 156, 8, 2283, 64, 35, 419, 24, 166, 82, 1531, 341, 2], 'False'), ([1, 12, 37, 146, 8, 51, 5, 4986, 31, 68, 12, 260, 2207, 2], 'True'), ([1, 30, 26, 91, 45, 5230, 7, 41, 34, 5, 330, 15, 1214, 2], 'True'), ([1, 54, 176, 21, 5, 10523, 17, 20, 5, 19090, 954, 3319, 7, 2], 'True'), ([1, 4609, 11849, 5563, 4641, 34, 3413, 471, 14, 9058, 22, 2899, 20783, 2], 'False'), ([1, 66, 2320, 10, 67, 15, 27, 158, 3, 9, 20, 3297, 6, 2], 'True'), ([1, 15996, 3322, 10, 8196, 1137, 3, 21175, 4496, 10, 2247, 23, 281, 2], 'False'), ([1, 14154, 647, 5433, 88, 216, 12394, 776, 9, 20378, 4, 3993, 3395, 2], 'True'), ([1, 12501, 22, 648, 9429, 2569, 4, 124, 14920, 3129, 11, 56, 154, 136, 2], 'False'), ([1, 2608, 5192, 95, 3201, 1588, 82, 513, 5, 13, 9154, 15, 1419, 13, 2], 'False'), ([1, 91, 54, 58, 4930, 307, 7531, 18, 11, 26, 20, 7531, 718, 439, 2], 'True'), ([1, 64, 1354, 9, 66, 178, 23, 11, 32, 257, 21, 9715, 5158, 7, 2], 'True'), ([1, 135, 997, 14, 5138, 7228, 55, 4, 258, 898, 585, 17538, 14, 16895, 2], 'False'), ([1, 12, 83, 70, 42, 438, 145, 491, 15, 1104, 9, 12275, 1308, 6, 2], 'True'), ([1, 3050, 1412, 117, 3037, 13, 13367, 13, 1039, 1814, 23, 1837, 1461, 11672, 2], 'False'), ([1, 2371, 22, 1192, 31045, 4002, 18, 16, 9, 4, 100, 2596, 2830, 7, 2], 'False'), ([1, 87, 1325, 2301, 3414, 8, 291, 29, 3707, 10, 801, 612, 24, 1669, 2], 'False'), ([1, 12, 45, 27, 130, 45, 12, 45, 27, 621, 31, 137, 262, 6, 2], 'True'), ([1, 104, 92, 12, 447, 5, 2164, 11275, 1765, 12, 196, 36, 9, 7598, 2], 'True'), ([1, 2447, 26, 1626, 36, 304, 43, 430, 1368, 8, 107, 156, 42, 267, 2], 'True'), ([1, 173, 92, 12, 99, 919, 2767, 5, 2085, 12, 99, 432, 975, 6, 2], 'True'), ([1, 16, 37, 3418, 2244, 22, 158, 569, 15, 9708, 7, 470, 3, 6, 2], 'True')]\n"
     ]
    }
   ],
   "source": [
    "# apply the bucket batch function on train data\n",
    "train_data_pipe = train_data_pipe.bucketbatch(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_num=5,\n",
    "    bucket_num=1,\n",
    "    use_in_batch_shuffle=False,\n",
    "    sort_key=sortBucket\n",
    ")\n",
    "\n",
    "print(list(train_data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([1, 638, 3484, 10519, 476, 16322, 215, 12662, 217, 2], [1, 4, 5345, 7463, 2840, 11, 6753, 122, 44, 2], [1, 12592, 7603, 5760, 14, 5, 10888, 1222, 2867, 2], [1, 15437, 19452, 342, 2512, 14, 4456, 6271, 554, 2], [1, 2229, 1008, 2572, 23, 890, 2807, 1152, 4599, 2], [1, 1324, 17, 23619, 3062, 6440, 208, 42, 9632, 2], [1, 4, 248, 1099, 15, 12589, 35, 218, 318, 2], [1, 1459, 609, 15, 27, 233, 17, 27, 108, 2], [1, 1094, 690, 2083, 2765, 77, 475, 3024, 2068, 2], [1, 4, 2527, 873, 1840, 4374, 62, 3022, 2624, 2], [1, 1084, 8608, 463, 1129, 23, 5, 6160, 4417, 2], [1, 1615, 2577, 5885, 36, 26, 362, 2163, 2640, 2], [1, 3005, 2980, 183, 187, 20261, 124, 1293, 5533, 2], [1, 35, 158, 357, 9967, 34, 147, 784, 218, 1838, 2], [1, 966, 28, 4, 1444, 1740, 3064, 9, 20, 1060, 2], [1, 246, 156, 2963, 55, 48, 398, 2182, 675, 57, 2], [1, 16, 19, 11, 47, 5, 10407, 3306, 7, 3, 2], [1, 49, 381, 9, 4, 118, 4368, 78, 71, 659, 2], [1, 16, 19, 11, 47, 5, 1073, 834, 7, 1723, 2], [1, 1690, 3, 2836, 23900, 14, 26614, 10, 576, 3295, 2], [1, 78, 364, 3255, 189, 1750, 255, 92, 15, 13540, 2], [1, 3149, 14524, 55, 1060, 16798, 1651, 23, 27, 4126, 2], [1, 19, 67, 282, 199, 88, 36, 3461, 11, 6, 2], [1, 4, 216, 124, 992, 2521, 154, 1462, 14, 1045, 2], [1, 30, 10300, 26, 66, 1534, 202, 17, 66, 1046, 2], [1, 48, 383, 444, 8, 27, 198, 7724, 570, 288, 2], [1, 4, 268, 1961, 2061, 430, 49, 7279, 23, 331, 2], [1, 687, 3146, 56, 933, 51, 5, 6427, 25, 237, 2], [1, 895, 824, 77, 3, 1518, 54, 37, 20, 6297, 2], [1, 4, 533, 478, 42, 725, 15, 7063, 163, 422, 2], [1, 53, 17840, 9, 5, 4462, 1141, 8, 1137, 7095, 2], [1, 216, 3047, 1372, 36, 61, 1245, 8, 51, 12177, 2], [1, 2359, 54, 2181, 1093, 7064, 80, 1566, 10256, 3401, 2], [1, 87, 1660, 8, 614, 5, 385, 10560, 708, 14, 589, 2], [1, 8, 4, 1947, 275, 5, 157, 53, 322, 22, 88, 2], [1, 12, 19, 71, 4, 272, 156, 4, 175, 138, 6, 2], [1, 416, 8035, 2151, 919, 25, 2121, 602, 16989, 14294, 973, 2], [1, 259, 6745, 10775, 524, 23, 1735, 22, 23780, 2667, 1839, 2], [1, 2555, 9688, 95, 40, 37, 267, 7939, 80, 4, 215, 2], [1, 11, 43, 14228, 48, 1017, 786, 2227, 25, 163, 860, 2], [1, 573, 3597, 12458, 3678, 98, 171, 6, 596, 1052, 708, 2], [1, 77, 409, 3585, 244, 422, 10, 4, 248, 174, 2746, 2], [1, 1756, 566, 18, 6598, 6235, 8, 113, 35, 2281, 309, 2], [1, 1547, 1661, 973, 403, 468, 8, 2662, 177, 2765, 1182, 2], [1, 87, 836, 77, 169, 14591, 80, 1855, 255, 1666, 1313, 2], [1, 32, 59, 3451, 81, 52, 366, 7, 6530, 21, 28, 2], [1, 16, 12, 2642, 55, 1316, 29, 27, 328, 17, 434, 2], [1, 600, 4176, 1098, 3170, 103, 332, 133, 498, 29, 1484, 2], [1, 30, 4, 4269, 1713, 15, 1997, 1981, 82, 21, 272, 2], [1, 1087, 5586, 3992, 133, 698, 628, 29, 17527, 234, 1376, 2], [1, 21, 9, 106, 132, 664, 8, 643, 65, 14081, 396, 2], [1, 10127, 275, 50, 18, 306, 1372, 744, 109, 137, 1050, 2], [1, 128, 59, 1120, 3294, 299, 7, 25, 4, 2368, 28, 2], [1, 5665, 105, 6600, 18, 12, 63, 90, 109, 5, 4571, 2], [1, 4431, 9, 48, 545, 10, 68, 40, 9, 27, 545, 2], [1, 1597, 1879, 3934, 6750, 8970, 8, 11380, 14467, 80, 699, 2], [1, 11299, 174, 16815, 18, 6746, 12165, 125, 1421, 1375, 7, 2], [1, 4, 4268, 403, 18, 2077, 3, 22, 23862, 29, 422, 2], [1, 16, 19, 11, 47, 5, 18282, 3703, 7, 14, 26696, 2], [1, 1024, 579, 1157, 14834, 1313, 29, 338, 24, 10299, 832, 2], [1, 46, 384, 11755, 2027, 5, 12603, 8, 478, 17, 907, 2], [1, 2391, 190, 15, 223, 22, 4459, 1834, 14, 145, 1454, 2], [1, 999, 1243, 36, 11, 26, 44, 8, 101, 86, 2039, 2], [1, 597, 1573, 1427, 24, 14, 25, 150, 1744, 98, 428, 2552, 2]), ('False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False'))\n"
     ]
    }
   ],
   "source": [
    "# convert the data the form: ((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))\n",
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets\n",
    "\n",
    "train_data_pipe = train_data_pipe.map(separateSourceTarget)\n",
    "print(list(train_data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding to the sentences\n",
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n",
    "## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
    "# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
    "# vocabulary.\n",
    "train_data_pipe = train_data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue from the section on making batches with bucket batch: https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"This utility function sanitizes a string by:\n",
    "    - removing links\n",
    "    - removing special characters\n",
    "    - removing numbers\n",
    "    - removing stopwords\n",
    "    - transforming in lowercase\n",
    "    - removing excessive whitespaces\n",
    "    Args:\n",
    "        text (str): the input text you want to clean\n",
    "    Returns:\n",
    "        str: the cleaned list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove special chars and numbers\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    # 1. tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # 2. check if stopword\n",
    "    tokens = [w.lower() for w in tokens if not w in stopwords.words(\"english\")]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of vocabulary\n",
    "\n",
    "def get_vocab(training_corpus):\n",
    "\n",
    "    # add special characters\n",
    "    # padding, end of line, unknown term\n",
    "    vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "    # build vocab from training corpus\n",
    "    for item in training_corpus: \n",
    "        processed_text = preprocess_text(item) # apply preprocessing on each text\n",
    "        for word in processed_text: # for each word in tokens\n",
    "          if word not in vocab: # if word not in vocab\n",
    "              vocab[word] = len(vocab) # create an entry in the vocab equal to the term, and its value is the length of the vocab \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the text data into a tensor\n",
    "def text_to_tensor(text: str, vocab_dict: dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Arguments: \n",
    "        text - string containing the text\n",
    "        vocab_dict - term vocabulary\n",
    "        unk_token - special char used to map the unknown items\n",
    "        verbose - print debug messages\n",
    "    Returns:\n",
    "        tensor_l - a tensor containing the indices of our terms\n",
    "    '''     \n",
    "    word_l = preprocess_text(text)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words in our text:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # initialize empty tensor\n",
    "    tensor_l = [] \n",
    "    \n",
    "    # take the __UNK__ value fro mthe vocabulary \n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"UNK has a value of {unk_ID}\")\n",
    "        \n",
    "    # for each word in the lsit:\n",
    "    for word in word_l:\n",
    "        # take the index\n",
    "        # if the word is not in vocab_dict, then assign UNK\n",
    "        word_ID = vocab_dict.get(word, unk_ID)\n",
    "        # append to tensor list\n",
    "        tensor_l.append(word_ID)\n",
    "\n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch this swimmer disappear into winter storm...</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they laughed at reagan, too: trump's ideas wil...</td>\n",
       "      <td>False</td>\n",
       "      <td>[9, 10, 11, 12, 13, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey, are you cold? go over to the corner, it i...</td>\n",
       "      <td>True</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cannot get a standing desk? these are almost a...</td>\n",
       "      <td>False</td>\n",
       "      <td>[21, 22, 23, 24, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want to hear a joke about my penis? never mind...</td>\n",
       "      <td>True</td>\n",
       "      <td>[26, 27, 28, 29, 30, 31, 32]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor  \\\n",
       "0  watch this swimmer disappear into winter storm...  False   \n",
       "1  they laughed at reagan, too: trump's ideas wil...  False   \n",
       "2  hey, are you cold? go over to the corner, it i...   True   \n",
       "3  cannot get a standing desk? these are almost a...  False   \n",
       "4  want to hear a joke about my penis? never mind...   True   \n",
       "\n",
       "                      tokenized  \n",
       "0            [3, 4, 5, 6, 7, 8]  \n",
       "1   [9, 10, 11, 12, 13, 14, 15]  \n",
       "2          [16, 17, 18, 19, 20]  \n",
       "3          [21, 22, 23, 24, 25]  \n",
       "4  [26, 27, 28, 29, 30, 31, 32]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tokenized'] = train_df['text'].apply(lambda x: text_to_tensor(x, vocab))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thought up a reddit joke today. when is a tria...</td>\n",
       "      <td>True</td>\n",
       "      <td>[2594, 4644, 28, 500, 6231, 25956, 11826]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how much do pirates pay for corn? a buck an ear!</td>\n",
       "      <td>True</td>\n",
       "      <td>[565, 2773, 1609, 450, 7213, 2974]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hillary clinton sent her book to every gop can...</td>\n",
       "      <td>False</td>\n",
       "      <td>[1156, 1157, 3507, 337, 602, 3108, 547, 2140, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>italian unions lambast new museum boss for wor...</td>\n",
       "      <td>False</td>\n",
       "      <td>[1874, 5630, 9196, 74, 3002, 6879, 999, 1901]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>life below the ocean’s surface wholly depends ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[150, 4423, 7975, 36527, 6120, 415]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor  \\\n",
       "0  thought up a reddit joke today. when is a tria...   True   \n",
       "1   how much do pirates pay for corn? a buck an ear!   True   \n",
       "2  hillary clinton sent her book to every gop can...  False   \n",
       "3  italian unions lambast new museum boss for wor...  False   \n",
       "4  life below the ocean’s surface wholly depends ...  False   \n",
       "\n",
       "                                           tokenized  \n",
       "0          [2594, 4644, 28, 500, 6231, 25956, 11826]  \n",
       "1                 [565, 2773, 1609, 450, 7213, 2974]  \n",
       "2  [1156, 1157, 3507, 337, 602, 3108, 547, 2140, 98]  \n",
       "3      [1874, 5630, 9196, 74, 3002, 6879, 999, 1901]  \n",
       "4                [150, 4423, 7975, 36527, 6120, 415]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['tokenized'] = test_df['text'].apply(lambda x: text_to_tensor(x, vocab))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # data loading\n",
    "        self.x = torch.from_numpy(np.array(df['tokenized']))\n",
    "        self.y = torch.from_numpy(np.array(df['humor'])) # converts the dataframe into a tensor\n",
    "\n",
    "    # this method needs an index\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m first \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m first\n",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# data loading\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumor\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_df)\n",
    "first = train_dataset[0]\n",
    "features, labels = first\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = CustomDataset(test_df)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:220\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Text: {X}\")\n",
    "    print(f\"Value: {y}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
